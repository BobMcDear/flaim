{"cells":[{"cell_type":"markdown","metadata":{"id":"TxhKRNAImM-b"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"yBlixGPDKUOh"},"source":["In this notebook, we will be training an image classifier on [the Oxford-IIIT Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) using flaim. The reader is assumed to be proficient at deep learning and familiar with JAX, Flax, and Optax, although this notebook might also serve as a helpful guide for those new to the JAX/Flax/Optax ecosystem who are seeking a basic training script free of more advanced ingredients like distributed or mixed-precision training.\n","\n","This notebook was run in Colab on a T4 GPU. The Colab notebook can be accessed [here](https://colab.research.google.com/drive/1U02GNjWUUmjLt9gTHvxOikYvGdA5qFIz?usp=share_link).\n","\n","_If you run out of memory during training, please restart the kernel and try again but conduct no training before the point at which you encounter the out-of-memory error._"]},{"cell_type":"markdown","metadata":{"id":"qJE25hUnAfLq"},"source":["# Dependencies"]},{"cell_type":"markdown","metadata":{"id":"V5wti7RTKR81"},"source":["Colab already comes with popular deep learning frameworks like PyTorch and JAX, and flaim is the only library that requires to be installed to train our Flax image classifier. We will also be re-implementing our code in PyTorch + timm to compare-and-contrast and will therefore need to install timm as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiSristO3WSJ"},"outputs":[],"source":["%%capture\n","!pip install git+https://github.com/BobMcDear/flaim.git\n","!pip install git+https://github.com/rwightman/pytorch-image-models.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwV8Ua8D3ddp"},"outputs":[],"source":["import typing as T\n","\n","# Flax + flaim training\n","import flaim\n","import jax\n","import optax\n","from flax.core.frozen_dict import FrozenDict\n","from flax.training.train_state import TrainState\n","from jax import numpy as jnp\n","\n","# PyTorch + timm training\n","import timm\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.optim import AdamW, Optimizer\n","from torch.optim.lr_scheduler import CosineAnnealingLR, _LRScheduler\n","\n","# Data loading\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import OxfordIIITPet"]},{"cell_type":"markdown","metadata":{"id":"ads63ExwB7TG"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"j3sEQYhvKXkx"},"source":["TensorFlow-based input pipelines tend to be more efficient than their PyTorch counterparts, especially in multi-worker settings, and it is recommended that JAX developers process data using TensorFlow. For the purpose of learning flaim though, either would do, and we shall opt for PyTorch data loaders thanks to their simplicity.\n","\n","Pre-processing will be kept to a minimum and consists of random resized cropping for the training set and resizing followed by a center crop for the validation set, in addition to converting the images to tensors and normalizing them. One caveat is that the input data must be channels-last JAX or NumPy arrays for Flax, so torchvision cannot be wholly relied upon. Instead, we'll write ```img_to_numpy``` for converting images to NumPy arrays, ```NumPyNormalize``` for normalizing them, and ```numpy_collate``` to be used as the collate function for the data loaders. Alternatively, we could've developed the data pipeline in pure PyTorch and converted the inputs and targets to JAX arrays during training on the fly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54EXmtmKi-7l"},"outputs":[],"source":["def img_to_numpy(img) -> np.ndarray:\n","  \"\"\"\n","  Converts an image to a float NumPy array with range [0, 1].\n","\n","  Args:\n","    img: Image to convert.\n","  \n","  Returns (np.ndarray): Image converted to a float NumPy array\n","  with range [0, 1]. \n","  \"\"\"\n","  return np.asarray(img)/255.\n","  \n","\n","class NumPyNormalize:\n","  \"\"\"\n","  Normalizes a NumPy array along the last axis.\n","\n","  Args:\n","    mean (T.Tuple[float, ...]): Mean for normalization.\n","    Default is (0.485, 0.456, 0.406).\n","    std (T.Tuple[float, ...]): Standard deviation for normalization.\n","    Default is (0.229, 0.224, 0.225).\n","  \"\"\"\n","  def __init__(\n","      self,\n","      mean: T.Tuple[float, ...] = (0.485, 0.456, 0.406),\n","      std: T.Tuple[float, ...] = (0.229, 0.224, 0.225),\n","      ) -> None:\n","      self.mean = np.array(mean)\n","      self.std = np.array(std)\n","    \n","  def __call__(self, input: np.ndarray) -> np.ndarray:\n","    return (input - self.mean) / self.std\n","\n","\n","def numpy_collate(\n","    batch: T.List[T.Tuple[np.ndarray, int]],\n","    ) -> T.List[np.ndarray]:\n","  \"\"\"\n","  Collates an input batch using NumPy for a PyTorch data loader.\n","\n","  Args:\n","    batch (T.List[T.Tuple[np.ndarray, int]]): Batch of samples to collate.\n","  \n","  Returns (T.List[np.ndarray]): Collated batch.\n","  \"\"\"\n","  transposed_batch = list(zip(*batch))\n","  return [np.stack(sample) for sample in transposed_batch]"]},{"cell_type":"markdown","metadata":{"id":"-ROWkLyAmy8P"},"source":["```get_pets_dls``` fetches the data loaders for us."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-VAvSIm4q8w"},"outputs":[],"source":["def get_pets_dls(\n","    root: str = '.',\n","    val_resize: int = 256,\n","    size: int = 224,\n","    numpy: bool = True,\n","    bs: int = 64,\n","    norm_mean: T.Tuple[float, ...] = (0.485, 0.456, 0.406),\n","    norm_std: T.Tuple[float, ...] = (0.229, 0.224, 0.225),\n","    ) -> T.Tuple[DataLoader, DataLoader]:\n","  \"\"\"\n","  Gets training and validation PyTorch data loaders for the Oxford-IIIT Pets dataset.\n","\n","  Args:\n","    root (str): Root directory for storing the dataset.\n","    Default is '.'\n","    val_resize (int): Size to which the validation set is resized to before\n","    being center cropped.\n","    Default is 256.\n","    size (int): Random resized crop size for the training set and center\n","    crop size for the validation set.\n","    Default is 224.\n","    numpy (bool): Whether the data should be returned as NumPy channels-last\n","    arrays. If False, it is returned as channels-first PyTorch tensors.\n","    Default is True.\n","    bs (int): Batch size.\n","    Default is 64.\n","    norm_mean (T.Tuple[float, ...]): Mean for normalization.\n","    Default is (0.485, 0.456, 0.406).\n","    norm_std (T.Tuple[float, ...]): Standard deviation for normalization.\n","    Default is (0.485, 0.456, 0.406).\n","  \n","  Returns (T.Tuple[DataLoader, DataLoader]): Training and validation data loaders.\n","  \"\"\"\n","  if numpy:\n","    to_tensor_and_normalize = (img_to_numpy, NumPyNormalize(norm_mean, norm_std))\n","    collate_fn = numpy_collate\n","  \n","  else:\n","    to_tensor_and_normalize = (transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std))\n","    collate_fn = None\n","\n","  train_ds = OxfordIIITPet(\n","      root='.',\n","      split='trainval',\n","      transform=transforms.Compose([\n","          transforms.RandomResizedCrop(size),\n","          *to_tensor_and_normalize,\n","          ]),\n","      download=True,\n","      )\n","  valid_ds = OxfordIIITPet(\n","      root='.',\n","      split='test',\n","      transform=transforms.Compose([\n","          transforms.Resize(val_resize),\n","          transforms.CenterCrop(size),\n","          *to_tensor_and_normalize,\n","          ]),\n","      download=True,\n","      )\n","\n","  train_dl = DataLoader(\n","      dataset=train_ds,\n","      batch_size=bs,\n","      shuffle=True,\n","      collate_fn=collate_fn,\n","      drop_last=True,\n","      )\n","  valid_dl = DataLoader(\n","      dataset=valid_ds,\n","      batch_size=bs,\n","      collate_fn=collate_fn,\n","      drop_last=True,\n","      )\n","  \n","  return train_dl, valid_dl"]},{"cell_type":"markdown","metadata":{"id":"qXx2qP1C6qNu"},"source":["# Training (Flax + flaim)"]},{"cell_type":"markdown","metadata":{"id":"FXROeAsw6sUY"},"source":["We are ready to train our classifier, which will be based off of [ConvNeXt](https://arxiv.org/abs/2201.03545), a convolutional neural network (CNN) that borrows ideas from the transformer literature for state-of-the-art visual recognition. To construct models, flaim provides [```flaim.get_model```](https://github.com/bobmcdear/flaim#usage), which returns a model, its parameters, and optionally the corresponding normalization statistics. The training process is more or less identical for other networks, one major exception being models that incorporate batch normalization (BN), which will be studied later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGiBtQYV7u-G"},"outputs":[],"source":["model, vars, norm_stats = flaim.get_model(\n","    model_name='convnext_small',\n","    pretrained='in1k_224',\n","    n_classes=37, # The number of breeds in the Pets dataset\n","    )"]},{"cell_type":"markdown","metadata":{"id":"s7yHf15PwbcL"},"source":["Using ```norm_stats```, we can create our data loaders."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M96YZIe7wk-B"},"outputs":[],"source":["%%capture\n","train_dl, valid_dl = get_pets_dls(\n","    norm_mean=norm_stats['mean'],\n","    norm_std=norm_stats['std'],\n","    )"]},{"cell_type":"markdown","metadata":{"id":"Cwonh2ET9clU"},"source":["Next, we set up an AdamW optimizer, with a cosine decay learning rate scheduler, using Optax. The scheduler requires the number of training iterations, which in turn depends on the number of epochs - we will  decide here to train for 5 epochs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WB1HFB9I-vFl"},"outputs":[],"source":["n_epochs = 5\n","lr = 6e-4\n","wd = 1e-2\n","\n","lr_scheduler = optax.cosine_decay_schedule(\n","  init_value=lr,\n","  decay_steps=n_epochs*len(train_dl), # Number of training iterations\n",")\n","optim = optax.adamw(\n","    learning_rate=lr_scheduler,\n","    weight_decay=wd,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"0qWo8Dr4xgSr"},"source":["Currently, the three major elements of training - the model, parameters, and optimizer - are completely decoupled, so we should bunch them together using Flax's ```TrainState``` class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtmEbPrnx8EG"},"outputs":[],"source":["state = TrainState.create(\n","    apply_fn=model.apply,\n","    params=vars,\n","    tx=optim,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"7gBXf5eVyMPL"},"source":["Finally, training can commence. The central component of our training script is ```train_iter```, a function that recieves ```state``` and a batch of inputs and targets, calculates the model's loss, and updates the parameters. ```train_iter``` itself contains an inner function, ```get_loss```, that only computes the cross-entropy loss, and JAX's ```value_and_grad``` automatically evaluates the gradients for us. Also, we just-in-time compile (JIT) our code for substantial performance boost."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUBdqwA_yWUh"},"outputs":[],"source":["@jax.jit\n","def train_iter(\n","    state: TrainState,\n","    input,\n","    target,\n","    ) -> T.Tuple[TrainState, float]:\n","  \"\"\"\n","  Calculates the model's loss on the current batch\n","  and updates its parameters.\n","\n","  Args:\n","    state (TrainState): State.\n","    input: Input.\n","    target: Target.\n","  \n","  Returns (T.Tuple[TrainState, float]): Updated state and loss.\n","  \"\"\"\n","  def get_loss(vars):\n","    output = state.apply_fn(vars, input)\n","    loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(output, target))\n","    return loss\n","  \n","  loss, grads = jax.value_and_grad(get_loss)(state.params)\n","  return state.apply_gradients(grads=grads), loss"]},{"cell_type":"markdown","metadata":{"id":"m23g1mHL1Qt4"},"source":["A similar function is necessary for validation. It differs from ```train_iter``` in that it does not update the parameters and also returns accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-ajPESI1c8g"},"outputs":[],"source":["@jax.jit\n","def valid_iter(\n","    state: TrainState,\n","    input,\n","    target,\n","    ) -> T.Tuple[float, float]:\n","  \"\"\"\n","  Calculates the model's loss and accuracy on the current batch.\n","\n","  Args:\n","    state (TrainState): State.\n","    input: Input.\n","    target: Target.\n","  \n","  Returns (T.Tuple[float, float]): Loss and accuracy.\n","  \"\"\"\n","  output = state.apply_fn(state.params, input)\n","  loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(output, target))\n","  accuracy = jnp.mean(jnp.argmax(output, axis=-1) == target)\n","  return loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"ehPsyTGG13hm"},"source":["The remainder is standard deep learning - a training epoch consists of iterating through each batch and updating the model, after which the model is validated on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnH4NQ27K3aS"},"outputs":[],"source":["def train_epoch(\n","    state: TrainState,\n","    train_dl: DataLoader,\n","    ) -> T.Tuple[TrainState, float]:\n","  \"\"\"\n","  Performs one training epoch.\n","\n","  Args:\n","    state (TrainState): State.\n","    train_dl (DataLoader): Training data loader.\n","  \n","  Returns (T.Tuple[TrainState, float]): Updated state and total loss.\n","  \"\"\"\n","  n_samples = 0\n","  loss = 0\n","\n","  for ind, (input, target) in enumerate(train_dl):\n","    if ind%10 == 0:\n","      print('\\r', end='')\n","      print(f'Training iteration {ind}/{len(train_dl)}', end='')\n","\n","    # jnp.array transfers to the data to the GPU automatically\n","    input, target = jnp.array(input), jnp.array(target)\n","    state, curr_loss = train_iter(state, input, target)\n","\n","    n_samples += len(input)\n","    loss += len(input) * curr_loss\n","  \n","  return state, loss/n_samples\n","\n","\n","def validate(\n","    state: TrainState,\n","    valid_dl: DataLoader,\n","    ) -> T.Tuple[float, float]:\n","  \"\"\"\n","  Validates the model.\n","\n","  Args:\n","    state (TrainState): State.\n","    valid_dl (DataLoader): Validation data loader.\n","  \n","  Returns (T.Tuple[float, float]): Total loss and accuracy.\n","  \"\"\"\n","  n_samples = 0\n","  loss = 0\n","  accuracy = 0\n","\n","  for ind, (input, target) in enumerate(valid_dl):\n","    if ind%10 == 0:\n","      print('\\r', end='')\n","      print(f'Validation iteration {ind}/{len(valid_dl)}', end='')\n","\n","    # jnp.array transfers to the data to the GPU automatically\n","    input, target = jnp.array(input), jnp.array(target)\n","    curr_loss, curr_accuracy = valid_iter(state, input, target)\n","\n","    n_samples += len(input)\n","    loss += len(input) * curr_loss\n","    accuracy += len(input) * curr_accuracy\n","  \n","  return loss/n_samples, accuracy/n_samples\n","\n","\n","def train(\n","    state: TrainState,\n","    train_dl: DataLoader,\n","    valid_dl: DataLoader,\n","    n_epochs: int = 5,\n","    ) -> TrainState:\n","  \"\"\"\n","  Trains model.\n","\n","  Args:\n","    state (TrainState): State.\n","    train_dl (DataLoader): Training data loader.\n","    valid_dl (DataLoader): Validation data loader.\n","    n_epochs (int): Number of epochs to train for.\n","    Default is 5.\n","  \n","  Returns (TrainState): Trained state.\n","  \"\"\"\n","  for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}')\n","\n","    state, train_loss = train_epoch(state, train_dl)\n","    print('\\r', end='')\n","    print(f'Training loss: {train_loss}')\n","\n","    valid_loss, valid_accuracy = validate(state, valid_dl)\n","    print('\\r', end='')\n","    print(f'Validation loss: {valid_loss}')\n","    print(f'Validation accuracy: {valid_accuracy}')\n","  \n","  return state"]},{"cell_type":"markdown","metadata":{"id":"-9IVsGHKijsf"},"source":["Let's train:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":707330,"status":"ok","timestamp":1675197299452,"user":{"displayName":"Borna Ahmadzadeh","userId":"18047404826117693986"},"user_tz":300},"id":"5YSKYiRXfbxN","outputId":"94aeb663-5d13-405c-c5b5-af5a6d60fdec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Training loss: 1.2170429229736328\n","Validation loss: 0.29066792130470276\n","Validation accuracy: 0.9125548601150513\n","Epoch 2\n","Training loss: 0.38544732332229614\n","Validation loss: 0.2744150757789612\n","Validation accuracy: 0.9163925647735596\n","Epoch 3\n","Training loss: 0.2568410038948059\n","Validation loss: 0.21699966490268707\n","Validation accuracy: 0.9317434430122375\n","Epoch 4\n","Training loss: 0.17679345607757568\n","Validation loss: 0.18962512910366058\n","Validation accuracy: 0.9413377046585083\n","Epoch 5\n","Training loss: 0.15397047996520996\n","Validation loss: 0.18540027737617493\n","Validation accuracy: 0.9440789818763733\n","CPU times: user 8min 33s, sys: 3min 11s, total: 11min 45s\n","Wall time: 11min 48s\n"]}],"source":["%%time\n","state = train(\n","    state=state,\n","    train_dl=train_dl,\n","    valid_dl=valid_dl,\n","    n_epochs=n_epochs,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"KK247sO-BpSx"},"source":["Training lasted roughly 12 minutes - as we will see, that is quite fast compared to PyTorch. It is important to bear in mind that the first epoch or so can be markedly slower because JAX is JITting ```train_iter``` and ```valid_iter``` for the first time. In the grand scheme of training, this extra overhead is typically negligible and is overshadowed by the actual training runtime, but one should be cognizant of it nonetheless."]},{"cell_type":"markdown","metadata":{"id":"jql9APQ1_AZW"},"source":["## Batch normalization"]},{"cell_type":"markdown","metadata":{"id":"z668VOAy_DaO"},"source":["Many networks contain layers such as batch normalization that exhibit different behaviour during training and inference. In PyTorch, modules have ```train``` and ```eval``` methods that signal to these submodules to enter training and inference mode respectively, but this strategy would not blend well with Flax's stateless nature. Consequently, Flax models - flaim included - oft-times accept a ```training``` argument in their ```apply``` method that places the network in training mode if ```True``` and in inference mode otherwise.\n","\n","Additionally, batch normalization involves a set of running means and variances, stored in the dictionary of parameters with key ```batch_stats```, that are updated not through backpropagation but as part of the forward pass. That would not normally be permitted since Flax modules are immutable, so we must inform Flax that this collection is mutable by setting ```mutable = ['batch_stats']``` in ```state.apply_fn```.\n","\n","In short, to adapt our code for models with BN, there are two adjustments we need to make. First, we must provide the appropriate values for ```training``` and ```mutable``` in each training or validation iteration to ensure the model is generating predictions correctly. Updating the model's parameters demands another modification: The training state, ```state```, cannot hold the running BN statistics in ```state.params``` because they are not updated through backpropagation; rather, the forward pass outputs the new means and variances, and one must manually replace the old ones with them. Hence, we will develop ```BNTrainState```, a child class of ```TrainState``` that has an additional ```batch_stats``` attribute for managing these means and variances. Note that ```train_epoch```, ```validate```, and ```train``` are not changed.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KwEyvLZFag0"},"outputs":[],"source":["class BNTrainState(TrainState):\n","  \"\"\"\n","  Training state with a batch_stats attribute for storing\n","  batch normalization statistics.\n","  \"\"\"\n","  batch_stats: FrozenDict\n","\n","\n","@jax.jit\n","def train_iter(\n","    state: TrainState,\n","    input,\n","    target,\n","    ) -> T.Tuple[TrainState, float]:\n","  \"\"\"\n","  Calculates the model's loss on the current batch\n","  and updates its parameters.\n","\n","  Args:\n","    state (TrainState): State.\n","    input: Input.\n","    target: Target.\n","  \n","  Returns (T.Tuple[TrainState, float]): Updated state and loss.\n","  \"\"\"\n","  def get_loss(params):\n","    vars = {'params': params, 'batch_stats': state.batch_stats}\n","    output, new_mutable_state = state.apply_fn(vars, input, training=True, mutable=['batch_stats'])\n","    loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(output, target))\n","    return loss, new_mutable_state\n","  \n","  (loss, new_mutable_state), grads = jax.value_and_grad(get_loss, has_aux=True)(state.params)\n","  return state.apply_gradients(grads=grads, batch_stats=new_mutable_state['batch_stats']), loss\n","\n","\n","@jax.jit\n","def valid_iter(\n","    state: TrainState,\n","    input,\n","    target,\n","    ) -> T.Tuple[float, float]:\n","  \"\"\"\n","  Calculates the model's loss and accuracy on the current batch.\n","\n","  Args:\n","    state (TrainState): State.\n","    input: Input.\n","    target: Target.\n","  \n","  Returns (T.Tuple[float, float]): Loss and accuracy.\n","  \"\"\"\n","  vars = {'params': state.params, 'batch_stats': state.batch_stats}\n","  output = state.apply_fn(vars, input, training=False, mutable=False)\n","  loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(output, target))\n","  accuracy = jnp.mean(jnp.argmax(output, axis=-1) == target)\n","  return loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"KGEjZLVUHTAA"},"source":["This time, we will be training ECA-ResNet, a derivative of ResNet that benefits from channel adaptibility through [efficient channel attention (ECA)](https://arxiv.org/abs/1910.03151). ECA-ResNet is much faster than ConvNeXt, so we will double the number of epochs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-7qCpunHlJr"},"outputs":[],"source":["%%capture\n","n_epochs = 10\n","lr = 4e-4\n","wd = 1e-2\n","\n","model, vars, norm_stats = flaim.get_model(\n","    model_name='ecaresnet50t',\n","    pretrained='in1k_256',\n","    n_classes=37, # The number of breeds in the Pets dataset\n","    )\n","\n","train_dl, valid_dl = get_pets_dls(\n","    norm_mean=norm_stats['mean'],\n","    norm_std=norm_stats['std'],\n","    )\n","\n","lr_scheduler = optax.cosine_decay_schedule(\n","  init_value=lr,\n","  decay_steps=n_epochs*len(train_dl), # Number of training iterations\n",")\n","optim = optax.adamw(\n","    learning_rate=lr_scheduler,\n","    weight_decay=wd,\n","    )\n","\n","state = BNTrainState.create(\n","    apply_fn=model.apply,\n","    params=vars['params'],\n","    batch_stats=vars['batch_stats'],\n","    tx=optim,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":737659,"status":"ok","timestamp":1675267033092,"user":{"displayName":"Borna Ahmadzadeh","userId":"18047404826117693986"},"user_tz":300},"id":"Uj_zglJeH4Fk","outputId":"c6e665d9-b6e9-4a78-a7c4-c2934604a9a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Training loss: 1.6559876203536987\n","Validation loss: 0.43099743127822876\n","Validation accuracy: 0.8637609481811523\n","Epoch 2\n","Training loss: 0.4938824772834778\n","Validation loss: 0.38565394282341003\n","Validation accuracy: 0.8780153393745422\n","Epoch 3\n","Training loss: 0.3837474584579468\n","Validation loss: 0.31008198857307434\n","Validation accuracy: 0.9018640518188477\n","Epoch 4\n","Training loss: 0.2917676270008087\n","Validation loss: 0.2872237265110016\n","Validation accuracy: 0.9084429740905762\n","Epoch 5\n","Training loss: 0.2586759328842163\n","Validation loss: 0.28441449999809265\n","Validation accuracy: 0.9125548601150513\n","Epoch 6\n","Training loss: 0.2273043394088745\n","Validation loss: 0.28030940890312195\n","Validation accuracy: 0.9152960777282715\n","Epoch 7\n","Training loss: 0.18541376292705536\n","Validation loss: 0.2546655833721161\n","Validation accuracy: 0.9207785129547119\n","Epoch 8\n","Training loss: 0.18228289484977722\n","Validation loss: 0.2613741457462311\n","Validation accuracy: 0.9191337823867798\n","Epoch 9\n","Training loss: 0.15694083273410797\n","Validation loss: 0.2547633945941925\n","Validation accuracy: 0.9199561476707458\n","Epoch 10\n","Training loss: 0.16683727502822876\n","Validation loss: 0.25416696071624756\n","Validation accuracy: 0.9232456088066101\n","CPU times: user 11min 16s, sys: 1min 14s, total: 12min 31s\n","Wall time: 12min 17s\n"]}],"source":["%%time\n","state = train(\n","    state=state,\n","    train_dl=train_dl,\n","    valid_dl=valid_dl,\n","    n_epochs=n_epochs,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"4ggwOASwUmXn"},"source":["92% accuracy in 12 minutes - not bad, but no match for a cutting-edge CNN like ConvNeXt. Note that signs of overfitting emerge in the latter half of learning, so you can experiment with increased regularization and augmentation for better generalization if you'd like. We shall train this network in PyTorch as well."]},{"cell_type":"markdown","metadata":{"id":"zcMuCU4lDixE"},"source":["# Training (PyTorch + timm)"]},{"cell_type":"markdown","metadata":{"id":"-p_a2M6snYEP"},"source":["Below is a PyTorch implementation of our application. The overall layout is the same, and this tutorial is not concerned with PyTorch anyhow, so we will not dive any deeper into it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RccvOat_ihhj"},"outputs":[],"source":["def train_iter(\n","    model: nn.Module,\n","    optim: Optimizer,\n","    input: torch.Tensor,\n","    target: torch.Tensor,\n","    scheduler: T.Optional[_LRScheduler] = None,\n","    ) -> float:\n","  \"\"\"\n","  Calculates the model's loss on the current batch\n","  and updates its parameters.\n","\n","  Args:\n","    model (nn.Module): Model.\n","    optim (Optimizer): Optimizer.\n","    input (torch.Tensor): Input.\n","    target (torch.Tensor): Target.\n","    scheduler (T.Optional[_LRScheduler]): Optional learning\n","    rate scheduler. If None, a constant learning rate is used.\n","    Default is None.\n","\n","  Returns (float): Loss.\n","  \"\"\"\n","  output = model(input)\n","  loss = F.cross_entropy(output, target)\n","  loss.backward()\n","\n","  optim.step()\n","  optim.zero_grad()\n","\n","  if scheduler:\n","    scheduler.step()\n","\n","  return loss.item()\n","\n","\n","def valid_iter(\n","    model: nn.Module,\n","    input: torch.Tensor,\n","    target: torch.Tensor,\n","    ) -> T.Tuple[float, float]:\n","  \"\"\"\n","  Calculates the model's loss and accuracy on the current batch.\n","\n","  Args:\n","    model (nn.Module): Model.\n","    input (torch.Tensor): Input.\n","    target (torch.Tensor): Target.\n","  \n","  Returns (T.Tuple[float, float]): Loss and accuracy.\n","  \"\"\"\n","  output = model(input)\n","  loss = F.cross_entropy(output, target)\n","  accuracy = (torch.argmax(output, dim=-1) == target).float().mean()\n","  return loss.item(), accuracy.item()\n","\n","\n","def train_epoch(\n","    model: nn.Module,\n","    optim: Optimizer,\n","    train_dl: DataLoader,\n","    scheduler: T.Optional[_LRScheduler] = None,\n","    ) -> float:\n","  \"\"\"\n","  Performs one training epoch.\n","\n","  Args:\n","    model (nn.Module): Model.\n","    optim (Optimizer): Optimizer.\n","    train_dl (DataLoader): Training data loader.\n","    scheduler (T.Optional[_LRScheduler]): Optional learning\n","    rate scheduler. If None, a constant learning rate is used.\n","    Default is None.\n","  \n","  Returns (float): Total loss.\n","  \"\"\"\n","  model.train()\n","  n_samples = 0\n","  loss = 0\n","\n","  for ind, (input, target) in enumerate(train_dl):\n","    if ind%10 == 0:\n","      print('\\r', end='')\n","      print(f'Training iteration {ind}/{len(train_dl)}', end='')\n","\n","    input, target = input.cuda(), target.cuda()\n","    curr_loss = train_iter(\n","        model=model,\n","        optim=optim,\n","        input=input,\n","        target=target,\n","        scheduler=scheduler,\n","        )\n","\n","    n_samples += len(input)\n","    loss += len(input) * curr_loss\n","  \n","  return loss/n_samples\n","\n","\n","def validate(\n","    model: nn.Module,\n","    valid_dl: DataLoader,\n","    ) -> T.Tuple[float, float]:\n","  \"\"\"\n","  Validates the model.\n","\n","  Args:\n","    model (nn.Module): Model.\n","    valid_dl (DataLoader): Validation data loader.\n","  \n","  Returns (T.Tuple[float, float]): Total loss and accuracy.\n","  \"\"\"\n","  model.eval()\n","  n_samples = 0\n","  loss = 0\n","  accuracy = 0\n","\n","  with torch.no_grad():\n","    for ind, (input, target) in enumerate(valid_dl):\n","      if ind%10 == 0:\n","        print('\\r', end='')\n","        print(f'Validation iteration {ind}/{len(valid_dl)}', end='')\n","\n","      input, target = input.cuda(), target.cuda()\n","      curr_loss, curr_accuracy = valid_iter(model, input, target)\n","\n","      n_samples += len(input)\n","      loss += len(input) * curr_loss\n","      accuracy += len(input) * curr_accuracy\n","  \n","  return loss/n_samples, accuracy/n_samples\n","\n","\n","def train(\n","  model: nn.Module,\n","  optim: nn.Module,\n","  train_dl: DataLoader,\n","  valid_dl: DataLoader,\n","  scheduler: T.Optional[_LRScheduler] = None,\n","  n_epochs: int = 5,\n","  ) -> nn.Module:\n","  \"\"\"\n","  Trains model.\n","\n","  Args:\n","    model (nn.Module): Model.\n","    optim (Optimizer): Optimizer.\n","    train_dl (DataLoader): Training data loader.\n","    valid_dl (DataLoader): Validation data loader.\n","    scheduler (T.Optional[_LRScheduler]): Optional learning\n","    rate scheduler. If None, a constant learning rate is used.\n","    Default is None.\n","    n_epochs (int): Number of epochs to train for.\n","    Default is 5.\n","  \n","  Returns (nn.Module): Trained model.\n","  \"\"\"\n","  for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}')\n","\n","    train_loss = train_epoch(\n","        model=model,\n","        optim=optim,\n","        train_dl=train_dl,\n","        scheduler=scheduler,\n","        )\n","    print('\\r', end='')\n","    print(f'Training loss: {train_loss}')\n","\n","    valid_loss, valid_accuracy = validate(model, valid_dl)\n","    print('\\r', end='')\n","    print(f'Validation loss: {valid_loss}')\n","    print(f'Validation accuracy: {valid_accuracy}')\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"eVacsFmPn34l"},"source":["Let's create ConvNeXt-Small with timm and train. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGmohuk7_juT"},"outputs":[],"source":["%%capture\n","n_epochs = 5\n","lr = 6e-4\n","wd = 1e-2\n","\n","model = timm.create_model(\n","    model_name='convnext_small.fb_in1k',\n","    pretrained=True,\n","    num_classes=37, # The number of breeds in the Pets dataset\n","    ).cuda()\n","optim = AdamW(\n","    params=model.parameters(),\n","    lr=lr,\n","    weight_decay=wd,\n","    )\n","\n","train_dl, valid_dl = get_pets_dls(\n","    numpy=False,\n","    norm_mean=model.default_cfg['mean'],\n","    norm_std=model.default_cfg['std'],\n","    )\n","\n","scheduler = CosineAnnealingLR(\n","    optimizer=optim,\n","    T_max=n_epochs*len(train_dl), # Number of training iterations\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1396783,"status":"ok","timestamp":1675091793742,"user":{"displayName":"Borna Ahmadzadeh","userId":"18047404826117693986"},"user_tz":300},"id":"rjyK_iIldgSK","outputId":"cfb38430-678f-49a5-e431-0b815055aed7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Training loss: 1.354935941466114\n","Validation loss: 0.3003519731001896\n","Validation accuracy: 0.9139254385964912\n","Epoch 2\n","Training loss: 0.3941231843149453\n","Validation loss: 0.27561301937359467\n","Validation accuracy: 0.9114583333333334\n","Epoch 3\n","Training loss: 0.2761638104392771\n","Validation loss: 0.220189911280677\n","Validation accuracy: 0.9322916666666666\n","Epoch 4\n","Training loss: 0.20301196985600287\n","Validation loss: 0.1899036405232261\n","Validation accuracy: 0.9377741228070176\n","Epoch 5\n","Training loss: 0.1642190142812436\n","Validation loss: 0.18487075274287348\n","Validation accuracy: 0.9413377192982456\n","CPU times: user 14min 2s, sys: 9min 13s, total: 23min 15s\n","Wall time: 23min 16s\n"]}],"source":["%%time\n","model = train(\n","    model=model,\n","    optim=optim,\n","    train_dl=train_dl,\n","    valid_dl=valid_dl,\n","    scheduler=scheduler,\n","    n_epochs=n_epochs,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"qE5aEts5bdIz"},"source":["Final accuracy is similar to what we obtained with Flax + flaim, but the latter was 2x as fast, which attests to the potential of JAX and JIT. Dramatic speedups like this, however, are not always realistic; in the following section, we will examine a case where JAX's speed gains are far more modest."]},{"cell_type":"markdown","metadata":{"id":"wPBws_OVOuf0"},"source":["## Batch normalization"]},{"cell_type":"markdown","metadata":{"id":"eHFI4VyROzGg"},"source":["Since PyTorch modules are stateful, no alterations are necessary to train models with batch normalization. We will reuse the code above to train ECA-ResNet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuK4Z05FPBVA"},"outputs":[],"source":["%%capture\n","n_epochs = 10\n","lr = 4e-4\n","wd = 1e-2\n","\n","model = timm.create_model(\n","    model_name='ecaresnet50t',\n","    pretrained=True,\n","    num_classes=37, # The number of breeds in the Pets dataset\n","    ).cuda()\n","optim = AdamW(\n","    params=model.parameters(),\n","    lr=lr,\n","    weight_decay=wd,\n","    )\n","\n","train_dl, valid_dl = get_pets_dls(\n","    numpy=False,\n","    norm_mean=model.default_cfg['mean'],\n","    norm_std=model.default_cfg['std'],\n","    )\n","\n","scheduler = CosineAnnealingLR(\n","    optimizer=optim,\n","    T_max=n_epochs*len(train_dl), # Number of training iterations\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":998665,"status":"ok","timestamp":1675268080797,"user":{"displayName":"Borna Ahmadzadeh","userId":"18047404826117693986"},"user_tz":300},"id":"X4UFpxo8XzRm","outputId":"75f398e5-1c63-4e41-d998-30a5911629a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","Training loss: 1.7298050559403604\n","Validation loss: 0.45278381732733625\n","Validation accuracy: 0.8552631578947368\n","Epoch 2\n","Training loss: 0.49622445435900436\n","Validation loss: 0.3360696582842553\n","Validation accuracy: 0.8928179824561403\n","Epoch 3\n","Training loss: 0.37976034469248954\n","Validation loss: 0.3354140745982397\n","Validation accuracy: 0.8939144736842105\n","Epoch 4\n","Training loss: 0.30646385644611557\n","Validation loss: 0.2665331764636855\n","Validation accuracy: 0.9141995614035088\n","Epoch 5\n","Training loss: 0.28369567124989997\n","Validation loss: 0.2571168984041402\n","Validation accuracy: 0.9177631578947368\n","Epoch 6\n","Training loss: 0.22420173778868557\n","Validation loss: 0.2767221699142012\n","Validation accuracy: 0.9183114035087719\n","Epoch 7\n","Training loss: 0.1982107046141959\n","Validation loss: 0.25447199858823105\n","Validation accuracy: 0.9202302631578947\n","Epoch 8\n","Training loss: 0.17676571939598051\n","Validation loss: 0.24388352461289942\n","Validation accuracy: 0.9229714912280702\n","Epoch 9\n","Training loss: 0.17798631664430886\n","Validation loss: 0.24588593888799087\n","Validation accuracy: 0.9243421052631579\n","Epoch 10\n","Training loss: 0.16659630116140633\n","Validation loss: 0.24251260721582202\n","Validation accuracy: 0.9257127192982456\n","CPU times: user 12min 59s, sys: 3min 37s, total: 16min 36s\n","Wall time: 16min 38s\n"]}],"source":["%%time\n","model = train(\n","    model=model,\n","    optim=optim,\n","    train_dl=train_dl,\n","    valid_dl=valid_dl,\n","    scheduler=scheduler,\n","    n_epochs=n_epochs,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"JDGLj-R5XZ_M"},"source":["Once again, the model's loss and accuracy are close to those of Flax + flaim, but PyTorch training was 30% slower - a noticeable gap, yet not on par with a 2x speedup. This should be a reminder that JAX performance boosts vary wildly depending on the network and can fall anywhere between being an order of magnitude faster or being virtually no better than PyTorch. Some other factors that influence the degree of speedup JAX offers include hardware, batch size, types of layers used, etc. Unfortunately, it is difficult to ascertain the extent of this speedup a priori, and the only way to accurately do so is by empirically benchmarking the code. "]},{"cell_type":"markdown","metadata":{"id":"T-L7kUTZLmlF"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"rw_ynPrrLniu"},"source":["In this notebook, we trained a ConvNeXt pet breed classifier using Flax + flaim, doing so twice as fast as an equivalent PyTorch + timm script whilst achieving comparable scores. Moreover, we also examined the special case of models with batch normalization, which require extra attention in Flax due to Flax's stateless philospophy. We trained ECA-ResNet as an example of a model with BN and discovered PyTorch was only 30% slower than JAX. Accordingly, we must be aware that despite JAX being generally more efficient than PyTorch, how much faster it is exactly is conditioned by, e.g., the network architecture, hardware, and so forth. "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["qJE25hUnAfLq","ads63ExwB7TG","qXx2qP1C6qNu","jql9APQ1_AZW","zcMuCU4lDixE","wPBws_OVOuf0","T-L7kUTZLmlF"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
